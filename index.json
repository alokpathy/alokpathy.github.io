[{"authors":["admin"],"categories":null,"content":"Hi, I\u0026rsquo;m Alok! I\u0026rsquo;m a 1st-year CS Ph.D. student at UC Berkeley, advised by Aydın Buluç and Kathy Yelick. I am affiliated with the PASSION Lab, the BeBOp Group, and Lawrence Berkeley National Lab.\nBroadly, I am interested in designing algorithms that leverage high-performance systems, particularly heterogeneous systems. I am currently interested in scaling graph-representation learning in distributed environments. In the past, I worked on accelerating graph analytics on GPUs. My work is supported by the NSF Fellowship.\nI am also passionate about science communication. To that end, I write for the Berkeley Science Review.\nBefore coming to Berkeley, I worked with Oded Green in David Bader\u0026rsquo;s HPC lab at Georgia Tech (Go Jackets!).\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"/author/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/admin/","section":"author","summary":"Hi, I\u0026rsquo;m Alok! I\u0026rsquo;m a 1st-year CS Ph.D. student at UC Berkeley, advised by Aydın Buluç and Kathy Yelick. I am affiliated with the PASSION Lab, the BeBOp Group, and Lawrence Berkeley National Lab.\nBroadly, I am interested in designing algorithms that leverage high-performance systems, particularly heterogeneous systems. I am currently interested in scaling graph-representation learning in distributed environments. In the past, I worked on accelerating graph analytics on GPUs. My work is supported by the NSF Fellowship.","tags":null,"title":"Alok Tripathy","type":"author"},{"authors":["Alok Tripathy","Katherine Yelick","Aydın Buluç"],"categories":[],"content":"","date":1589049814,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589049814,"objectID":"62202cca4ffe07b9dafa8dfc81a2f64d","permalink":"/publication/2020-sc-gnn/","publishdate":"2020-05-09T14:43:34-04:00","relpermalink":"/publication/2020-sc-gnn/","section":"publication","summary":"Graph Neural Networks (GNNs) are powerful and flexible neural networks that use the naturally sparse connectivity information of the data. GNNs represent this connectivity as sparse matrices, which have lower arithmetic intensity and thus higher communication costs compared to dense matrices, making GNNs harder to scale to high concurrencies than convolutional or fully-connected neural networks. We present a family of parallel algorithms for training GNNs. These algorithms are based on their counterparts in dense and sparse linear algebra, but they had not been previously applied to GNN training. We show that they can asymptotically reduce communication compared to existing parallel GNN training methods. We implement a promising and practical version that is based on 2D sparse-dense matrix multiplication using torch.distributed. Our implementation parallelizes over GPU-equipped clusters. We train GNNs on up to a hundred GPUs on datasets that include a protein network with over a billion edges.","tags":["Graph-Representation Learning"],"title":"Reducing Communication in Graph Neural Network Training","type":"publication"},{"authors":["James Fox","Alok Tripathy","Oded Green"],"categories":[],"content":"","date":1569437014,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569437014,"objectID":"df3e1d37a373f274633c1f3db5628c04","permalink":"/publication/2019-hpec-lrb/","publishdate":"2019-09-25T14:43:34-04:00","relpermalink":"/publication/2019-hpec-lrb/","section":"publication","summary":"Effective scheduling and load balancing of applications on massively multi-threading systems remains challenging despite decades of research, especially for irregular and data dependent problems where the execution control path is unknown until run-time. One of the most widely used loadbalancing schemes used for data dependent problems is a parallel prefix sum (PPS) array over the expected amount of work per task, followed by a partitioning of tasks to threads. While sufficient for many systems, it is not ideal for massively multithreaded systems with SIMD/SIMT execution, such as GPUs.  More fine-grained load-balancing is needed to effectively utilize SIMD/SIMT units. In this paper we introduce Logarithmic Radix Binning (LRB) as a more suitable alternative to parallel prefix summation for load-balancing on such systems. We show that LRB has better scalability than PPS for high thread counts on Intel’s Knight’s Landing processor and comparable scalability on NVIDIA Volta GPUs. On the application side, we show how LRB improves the performance of PageRank up to 1.75X using the branch-avoiding model. We also show how to better load-balance segmented sort and improve performance on the GPU.","tags":["Graph Analytics"],"title":"Improving Scheduling for Irregular Applications with Logarithmic Radix Binning","type":"publication"},{"authors":null,"categories":[],"content":"","date":1553107800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553107800,"objectID":"a126027c17dacd87034d8068618ea8b2","permalink":"/talk/2019-gtc/","publishdate":"2019-03-20T15:00:00-03:50","relpermalink":"/talk/2019-gtc/","section":"talk","summary":"","tags":["Accelerators","Graph Analytics"],"title":"Scalable K-Core Decomposition for Static Graphs Using a Dynamic Graph Data Structure","type":"talk"},{"authors":["Alok Tripathy","Fred Hohman","Duen Horng Chau","Oded Green"],"categories":[],"content":"","date":1544711896,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544711896,"objectID":"c1062d764c6be49868edced6e80f70c9","permalink":"/publication/2018-bigdata-kcore/","publishdate":"2018-12-13T10:38:16-04:00","relpermalink":"/publication/2018-bigdata-kcore/","section":"publication","summary":"The k-core of a graph is a metric used in a wide range of applications, including social networks analytics, visualization, and graph coloring. Finding the maximal k-core of a graph can be be done in near linear time. The low computational requirements for finding the maximal k-core makes effective parallelization challenging, especially for the iterative algorithms that prune vertices and edges that no longer meet the requirements of the maximal k-core and require rebuilding the graph every iteration. In this paper, we present a new parallel and scalable algorithm for finding the maximal k-core. Similar to past algorithms, our algorithm also prunes vertices and edges. Unlike past approaches, our new algorithm does not rebuild the graph in every iteration-rather, it uses a dynamic graph data structure and avoids one of the largest performance penalties of k-core. We also show how to extend our algorithm to support k-core edge decomposition for different size k-cores found in the graph. This can be used for visualization and community analysis. While our new algorithms are architecture independent, our implementations target NVIDIA GPUs. When comparing our algorithms against several highly optimized algorithms, including the sequential igraph implementation and the multi-thread ParK implementation, our new algorithms are significantly faster. For finding the maximal k-core in the graph, our new algorithm can be up-to 58× faster the igraph and up-to 4× faster than ParK executed on a 36 core (72 thread) system. For the k-core decomposition algorithm, we saw even greater and more consistent speedups for our algorithm where it was up-to 130× faster than igraph and up-to 8× faster than ParK. Our algorithms were executed on an NVIDIA P100 GPU.","tags":["Graph Analytics"],"title":"Scalable K-Core Decomposition for Static Graphs Using a Dynamic Graph Data Structure","type":"publication"},{"authors":["Oded Green","James Fox","Alex Watkins","Alok Tripathy","Kasimir Gabert","Euna Kim","Xiaojing An","Kumar Aatish","David A. Bader"],"categories":[],"content":"","date":1537901014,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537901014,"objectID":"63901c5c7cf92c77ece668d5fc21a44d","permalink":"/publication/2018-hpec-lrb/","publishdate":"2018-09-25T14:43:34-04:00","relpermalink":"/publication/2018-hpec-lrb/","section":"publication","summary":"Triangle counting is a building block for numerous graph applications and given the fact that graphs continue to grow in size, its scalability is important. As such, numerous algorithms have been designed for triangle counting-some of which are compute-bound rather than memory bound. Even for compute-bound algorithms, one of the key challenges is the limited control flow available on the processor. This is in-part due to the high dependency between the control flow, input data, and limited utilization of vector instructions. Not surprising, compilers are not always able to detect these data dependencies and vectorize the algorithms. Using the branch-avoiding model we show to remove control flow restrictions by replacing branches with an equivalent set of arithmetic operations. More so, we show how these can be vectorized using Intel's AVX-512 instruction set and that our new vectorized algorithms are 2 × −5× faster than scalar counterparts. We also present a new load balancing method, Logarithmic Radix Binning (LRB) that ensures that threads and the vector data lanes execute a near equal amount of work at any given time. Altogether, our algorithm outperforms several 2017 HPEC Graph Challenge Champions such as the KOKKOS framework and a GPU based algorithm by anywhere from 1.5× and up to 14×.","tags":["Graph Analytics"],"title":"Logarithmic Radix Binning and Vectorized Triangle Counting","type":"publication"},{"authors":["Alok Tripathy","Oded Green"],"categories":[],"content":"","date":1537901014,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537901014,"objectID":"b3de427adb92266c21674859dee288f1","permalink":"/publication/2018-hpec-sbc/","publishdate":"2018-09-25T14:43:34-04:00","relpermalink":"/publication/2018-hpec-sbc/","section":"publication","summary":"The Betweenness Centrality of a vertex is an important metric used for determining how “central” a vertex is in a graph based on the number of shortest paths going through that vertex. Computing the betweenness centrality of a graph is computationally expensive, O(V ·(V +E)). This has led to the development of several important optimizations includ- ing: approximation, parallelization, and dealing with dynamic updates. Dynamic graph algorithms are extremely favorable as the amount of work that they require is orders of magnitude smaller than their static graph counterparts. Recently, several such dynamic graph algorithms for betweenness centrality have been introduced. Many of these new dynamic graph algorithms tend to have decent parallel scalability when the betweenness centrality metric is computed in an exact manner. However, for the cases where the approximate solution is used, the scalability drops because of bad load-balancing due to the reduction in the amount of work. In this paper, we show a dynamic graph betweenness centrality algorithm that has good parallel scalability for both exact and approximate computations. We show several new optimizations made to the data structures, the load balancing technique, and the parallel granularity that have improved overall performance to 1.6X − 4X faster than one of the fastest previous implementations. More so, our new algorithm scales to larger thread counts than before.","tags":["Graph Analytics"],"title":"Scaling Betweenness Centrality in Dynamic Graphs","type":"publication"}]
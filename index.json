[{"authors":["admin"],"categories":null,"content":"Hi, I\u0026rsquo;m Alok! I\u0026rsquo;m an incoming CS Ph.D. student at UC Berkeley and Graduate Research Assistant at Lawrence Berkeley National Lab advised by Aydın Buluç and Kathy Yelick.\nBroadly, I am interested in designing algorithms and programming models that leverage high-performance systems, particularly heterogeneous systems. My work is supported by the NSF Fellowship.\nBefore coming to Berkeley, I worked with Oded Green in David Bader\u0026rsquo;s HPC lab at Georgia Tech (Go Jackets!).\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"Hi, I\u0026rsquo;m Alok! I\u0026rsquo;m an incoming CS Ph.D. student at UC Berkeley and Graduate Research Assistant at Lawrence Berkeley National Lab advised by Aydın Buluç and Kathy Yelick.\nBroadly, I am interested in designing algorithms and programming models that leverage high-performance systems, particularly heterogeneous systems. My work is supported by the NSF Fellowship.\nBefore coming to Berkeley, I worked with Oded Green in David Bader\u0026rsquo;s HPC lab at Georgia Tech (Go Jackets!","tags":null,"title":"Alok Tripathy","type":"author"},{"authors":null,"categories":[],"content":"","date":1553107800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553107800,"objectID":"a126027c17dacd87034d8068618ea8b2","permalink":"/talk/2019-gtc/","publishdate":"2019-03-20T15:00:00-03:50","relpermalink":"/talk/2019-gtc/","section":"talk","summary":"","tags":["Accelerators","Graph Analytics"],"title":"Scalable K-Core Decomposition for Static Graphs Using a Dynamic Graph Data Structure","type":"talk"},{"authors":["Alok Tripathy","Fred Hohman","Duen Horng Chau","Oded Green"],"categories":[],"content":"","date":1544711896,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544711896,"objectID":"c1062d764c6be49868edced6e80f70c9","permalink":"/publication/2018-bigdata-kcore/","publishdate":"2018-12-13T10:38:16-04:00","relpermalink":"/publication/2018-bigdata-kcore/","section":"publication","summary":"The k-core of a graph is a metric used in a wide range of applications, including social networks analytics, visualization, and graph coloring. Finding the maximal k-core of a graph can be be done in near linear time. The low computational requirements for finding the maximal k-core makes effective parallelization challenging, especially for the iterative algorithms that prune vertices and edges that no longer meet the requirements of the maximal k-core and require rebuilding the graph every iteration. In this paper, we present a new parallel and scalable algorithm for finding the maximal k-core. Similar to past algorithms, our algorithm also prunes vertices and edges. Unlike past approaches, our new algorithm does not rebuild the graph in every iteration-rather, it uses a dynamic graph data structure and avoids one of the largest performance penalties of k-core. We also show how to extend our algorithm to support k-core edge decomposition for different size k-cores found in the graph. This can be used for visualization and community analysis. While our new algorithms are architecture independent, our implementations target NVIDIA GPUs. When comparing our algorithms against several highly optimized algorithms, including the sequential igraph implementation and the multi-thread ParK implementation, our new algorithms are significantly faster. For finding the maximal k-core in the graph, our new algorithm can be up-to 58× faster the igraph and up-to 4× faster than ParK executed on a 36 core (72 thread) system. For the k-core decomposition algorithm, we saw even greater and more consistent speedups for our algorithm where it was up-to 130× faster than igraph and up-to 8× faster than ParK. Our algorithms were executed on an NVIDIA P100 GPU.","tags":["Graph Analytics"],"title":"Scalable K-Core Decomposition for Static Graphs Using a Dynamic Graph Data Structure","type":"publication"},{"authors":["Oded Green","James Fox","Alex Watkins","Alok Tripathy","Kasimir Gabert","Euna Kim","Xiaojing An","Kumar Aatish","David A. Bader"],"categories":[],"content":"","date":1537901014,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537901014,"objectID":"63901c5c7cf92c77ece668d5fc21a44d","permalink":"/publication/2018-hpec-lrb/","publishdate":"2018-09-25T14:43:34-04:00","relpermalink":"/publication/2018-hpec-lrb/","section":"publication","summary":"Triangle counting is a building block for numerous graph applications and given the fact that graphs continue to grow in size, its scalability is important. As such, numerous algorithms have been designed for triangle counting-some of which are compute-bound rather than memory bound. Even for compute-bound algorithms, one of the key challenges is the limited control flow available on the processor. This is in-part due to the high dependency between the control flow, input data, and limited utilization of vector instructions. Not surprising, compilers are not always able to detect these data dependencies and vectorize the algorithms. Using the branch-avoiding model we show to remove control flow restrictions by replacing branches with an equivalent set of arithmetic operations. More so, we show how these can be vectorized using Intel's AVX-512 instruction set and that our new vectorized algorithms are 2 × −5× faster than scalar counterparts. We also present a new load balancing method, Logarithmic Radix Binning (LRB) that ensures that threads and the vector data lanes execute a near equal amount of work at any given time. Altogether, our algorithm outperforms several 2017 HPEC Graph Challenge Champions such as the KOKKOS framework and a GPU based algorithm by anywhere from 1.5× and up to 14×.","tags":["Graph Analytics"],"title":"Logarithmic Radix Binning and Vectorized Triangle Counting","type":"publication"},{"authors":["Alok Tripathy","Oded Green"],"categories":[],"content":"","date":1537901014,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537901014,"objectID":"b3de427adb92266c21674859dee288f1","permalink":"/publication/2018-hpec-sbc/","publishdate":"2018-09-25T14:43:34-04:00","relpermalink":"/publication/2018-hpec-sbc/","section":"publication","summary":"The Betweenness Centrality of a vertex is an important metric used for determining how “central” a vertex is in a graph based on the number of shortest paths going through that vertex. Computing the betweenness centrality of a graph is computationally expensive, O(V ·(V +E)). This has led to the development of several important optimizations includ- ing: approximation, parallelization, and dealing with dynamic updates. Dynamic graph algorithms are extremely favorable as the amount of work that they require is orders of magnitude smaller than their static graph counterparts. Recently, several such dynamic graph algorithms for betweenness centrality have been introduced. Many of these new dynamic graph algorithms tend to have decent parallel scalability when the betweenness centrality metric is computed in an exact manner. However, for the cases where the approximate solution is used, the scalability drops because of bad load-balancing due to the reduction in the amount of work. In this paper, we show a dynamic graph betweenness centrality algorithm that has good parallel scalability for both exact and approximate computations. We show several new optimizations made to the data structures, the load balancing technique, and the parallel granularity that have improved overall performance to 1.6X − 4X faster than one of the fastest previous implementations. More so, our new algorithm scales to larger thread counts than before.","tags":["Graph Analytics"],"title":"Scaling Betweenness Centrality in Dynamic Graphs","type":"publication"}]